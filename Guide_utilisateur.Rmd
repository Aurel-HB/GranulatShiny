---
output:
  html_document:
    toc: true
    toc_float: true
  pdf_document:
    toc: true
  word_document:
    toc: true
---

```{=html}
<style>
body {
text-align: justify}
</style>
```
# <i class="fa-solid fa-book"></i> Guide utilisateur de GranulatShiny

Auteurs : Aurel Hebert--Burggraeve, Laure Simplet, Laurent Dubroca, Camille Vogel <br>

```{r, echo=FALSE,  out.width = "50%", fig.align = "left"}
knitr::include_graphics("inst/app/www/favicon.png")
```

## <i class="fa-solid fa-dungeon"></i> Page d'accueil

```{r, echo=FALSE, warning=FALSE, results = FALSE, message=FALSE}
library(shiny)
library(DT)
library(ggplot2)
library(cowplot)
library(dplyr)
library(leaflet)
library(RColorBrewer)
library(lme4)
library(DHARMa)
library(shinydashboard)
library(ggeffects)
library(vegan)
library(pairwiseAdonis) #library(devtools) ; install_github("pmartinezarbizu/pairwiseAdonis/pairwiseAdonis")
```

<br> GranulatShiny est une application qui facilite le traitement statistique des données collectées dans le cadre des états initiaux, états de référence avant travaux et suivis environnementaux dédiés aux ressources halieutiques et à l’ichtyofaune relatifs à l'instruction des demandes d'autorisation de travaux d’extraction de granulats marins. L’application permet d’automatiser une partie de la mise en forme des données et du calcul d’indicateurs standards de biodiversité, et fournir des clés de décision pour les étapes plus avancées du traitement.<br> À partir des indicateurs calculés et des choix faits par l’utilisateur, l’application produit des figures et des tableaux dans des formats correspondant aux recommandations du "Protocole halieutique" et du guide méthodologique pour l’élaboration des Documents d’Orientation pour une Gestion durable des Granulats Marins (DOGGM). L’application fournit une interface graphique interactive basée sur le langage R dispensant l’utilisateur de sa maitrise pour se concentrer sur les paramètres d’intérêt pour le diagnostic des effets potentiels de l’extraction de granulats marins sur les ressources halieutiques. GranulatShiny est composée de 3 approches statistiques (exploaratoire, descriptive et inférentielle) qui peuvent être utilisées principalement pour quantifier l’influence de l’extraction de granulats marins sur les communautés de poissons. Chaque approche va couvrir un attendu du "Protocole halieutique". L’approche dite exploratoire présente et analyse les données à l’échelle de la communauté entière. L’approche descriptive présente et analyse les données à l’échelle d’une espèce. Et le recours à de l’analyse inférentielle est nécessaire pour évaluer la variabilité temporelle et spatiale des différents indicateurs des ressources halieutiques avant et pendant l'exploitation.<br> L’application dans sa version actuelle n’a pas de finalité réglementaire mais représente une aide pour la réalisation des rapports de suivi sur les communautés halieutiques. Elle ne vient pas remplacer le travail déjà fourni par les bureaux d’études mais vient compléter ce qui est déjà réalisé en permettant le montage d’un modèle statistique pour tester les effets de certains paramètres sur les communautés de poissons.

<br>Ce guide a été rédigé afin de permettre à un utilisateur de GranulatShiny de s'approprier l'interface de l'application et de comprendre la méthodologie développée derrière chaque résultat fourni par l'application. <br> Avant de commencer, notez qu'il existes différents boutons dans l'application. <br>

Les boutons avec une icône bateau vous permettent de **passer d'un onglet à un autre**.

```{r, echo=FALSE, warning=FALSE}
actionButton("start", "start", icon = icon("ship"))
```

Ceux portant un petit dragon vert désignent une **étape obligatoire**.

```{r, echo=FALSE, warning=FALSE}
actionButton("go", "Mettre en forme",icon = icon("dragon", style='color: #22A433'))
```

Les boutons avec une flèche permettent de **télécharger des résultats** de l'application. Les formats utilisés sont (csv, png, txt, rds).

```{r, echo=FALSE, warning=FALSE}
actionButton("tel", "Telecharger la table",icon = icon("download"))
```

Enfin, ceux avec un cercle contenant un i sont des **aides** que l'on peut faire afficher pour mieux comprendre un graphique ou autres objets proposés par l'application.

```{r, echo=FALSE, warning=FALSE}
actionButton("info", "",icon = icon("circle-info"))
```

Lors du démarrage de l'application, la page d'accueil s'ouvre automatiquement. Sur cette page, les différents documents de références et d'informations sont listés avec leur URL associée en hyperlien. Un rappel du contexte sur l'extraction de granulats marins se trouve à droite de la page. <br> Pour passer à l'onglet suivant appuyer sur le bouton start de la page d'accueil dans l'application.

## <i class="fa-solid fa-wand-magic-sparkles"></i> Mise en forme des données

###  Informations à rentrer : chargement des jeux de données à analyser

La première étape d'utilisation de l'application est l'importation des données collectées. Celles-ci doivent répondre à un standard défini par l’Ifremer et dont le canevas peut être retrouvé ici : <br>
http://htmlpreview.github.io/?https://github.com/Aurel-HB/GranulatShiny/blob/main/Description_Format_Generique_GranulatShiny.html
<br> 

Trois cas de figures peuvent se présenter : <br>

**- Cas n°1 : vous découvrez l’outil et n’avez pas de données formatées pour tester ses fonctionnalités. Vous vous appuierez alors sur le jeu de données fourni avec l’outil**<br> S'il s'agit d'un exercice de découverte de l'outil, un jeu de données factice est mis à disposition de l'utilisateur avec l'outil. Les données sont directement intégrées dans l'application et peuvent être chargées en sélectionnant la réponse non sous l'entête: "Avez vous vos propres données ?". Ce jeu de données a une vocation pédagogique, il ne correspond à aucun cas réel et ne peut donc apparaître dans des documents à valeur administrative (i.e. rapports de suivi, état initial de l'environnement, état de référence avant travaux...).

Pour les besoins de la prise en main de l’outil, le jeu de données mis à disposition avec l’outil correspond à une concession fictive qui serait située dans le Golfe de Gascogne. Pour ce jeu de données fictif, on considère une concession en exploitation de 2000 à 2030, pour laquelle un suivi du compartiment halieutique a été mis en place tous les 5 ans avec 2 années d’état initial. Le plan d’échantillonnage fictif prévoit l’échantillonnage de 10 stations dans la concession et 10 stations hors de la concession. Ce choix ne correspond pas à un plan d’échantillonnage qui aurait été élaboré en connaissance des conditions environnementales du site (i.e. faciès sédimentaires, habitats benthiques) et ne correspond donc pas aux recommandations du "Protocole halieutique". Ce plan d’échantillonnage fictif utilise un chalut à perche de 4,4 m d’ouverture horizontale avec des traits d’une longueur 1000 mètres. <br> Pour éviter toute confusion, les 4 espèces présentes dans ce jeu de données sont fictives également. Chaque espèce possède une dynamique de population associée à une loi de distribution de probabilité spécifique. Sachant cela, il est possible de contrôler les résultats issus des statistiques inférentielles et les effets de l’environnement sur l’espèce choisie. La première espèce Cephalaspis.tenuicornis n’est pas impactée par l’extraction et sa dynamique spatio-temporelle est stable sur l'ensemble du suivi (i.e. il n'y a pas d'effet du temps, de l’espace ou des conditions environnementales sur les abondances observées). Ainsi aucun effet potentiel des variables sur l’abondance de cette espèce ne sera détecté. L’espèce Dimichtys.terreli est impactée par l’extraction mais sa dynamique spatio-temporelle est stable sur l'ensemble du suivi. On a donc un effet significatif de l'extraction, qui se traduit par une différence entre les valeurs obtenues par l'échantillonnage dans la zone ou hors de la zone d’exploitation. Leedsischthys.problematicus est impactée par l’extraction mais différemment en fonction des saisons. Les saisons n’influencent pas la population de cette espèce en temps normal, c'est à dire en l'absence d'extraction de granulats, mais l’interaction entre l'effet de l'extraction et l'effet saisonnier modifie l’abondance de cette espèce. Enfin, Latimeria.chalumnae n'est impacté ni par l'effet de l'extraction, ni par des effets spatio-temporels tels que la saison et/ou les conditions environnementales, mais l'abondance de l'espèce est naturellement très variable. Ces exemples permettent d’illustrer des réponses à l’environnement différentes pour mieux comprendre ce qui est recherché lors de l’analyse inférentielle.<br>

**- Cas n°2 : vous avez une première expérience de l’outil et vous vous lancez dans l’analyse de votre propre jeu de données mis au format recommandé** <br> S’il s’agit d’un travail d’analyse de données réelles mises au format adéquat, vous devez sélectionner et charger dans l’interface graphique GranulatShiny les fichiers : « TuttiCatch.csv » et « TuttiOperation.csv » qui rassemblement l’essentiel des informations relatives au déroulé et aux résultats des suivis mis en œuvre. Seul le format csv est pris en charge. Le fichier « TuttiCatch.csv » correspond aux données de captures provenant de l’échantillonnage des populations de poissons et le fichier « TuttiOperation.csv » correspond à toutes les informations dérivées de la mise en œuvre du protocole pour chaque station d’échantillonnage (i.e. date de réalisation de la campagne et nom de celle-ci, engin de pêche dont les caractéristiques seront par ailleurs spécifiées dans les rapports associés au rendu des résultats, coordonnées géographiques des point de filage et de virage et horaires associés, durée totale du trait de chalut, profondeur de virage et de filage).<br> **ATTENTION**. Le format de données attendu doit être respecté sans quoi les routines de traitement ne peuvent se lancer correctement et un message d’avertissement apparaîtra sur l’interface. Dans ce cas, il est recommandé de revoir le format de votre fichier avec le format de fichier attendu. De plus, dans le cas de sous-échantillonnage ou d'une information fournie au niveau individuel, il est important de rapporter l'information à l'échelle du trait pour qu'à chaque combinaison d'espèce et de trait de chalut corresponde une seule ligne dans le tableau de données. Sinon un message sera renvoyé indiquant l’existence de doublons empêchant le traitement des données de captures. <br>

Une fois les fichiers chargés, vous aurez accès à de nouvelles fonctionnalités. Une carte centrée sur la concession apparaîtra avec un affichage des stations d’échantillonnage. Vous aurez également la possibilité d’interagir avec les champs "Stations d’impact" et Stations de référence. Vous aurez aussi la possibilité d’importer des fichiers de type « ShapeFiles » pour afficher les contours de la concession d’extraction de granulats marins. <br> Sous l’entête “Stations d’impact”, vous pouvez vérifier et modifier la période d’exploitation (période durant laquelle des travaux d'extractions ont lieu). Vous devez également écrire dans l’espace correspondant les stations qui sont impactées par l’extraction. La couleur des différents prélèvements devient alors rouge pour les stations impactées (figure ci-dessous).<br> Conformément au paragraphe 8.3.2 du “Protocole halieutique”, l’application est développée pour le cas le plus courant d’un échantillonnage au chalut. Ainsi, il faut rentrer la longueur d’ouverture horizontale du chalut pour permettre le calcul des surfaces échantillonnées afin de travailler en densité. A ce stade de développement, l’application ne prend pas en compte d’autres engins de pêches.<br> Enfin, dans le cas exceptionnel où une station rentrée dans le fichier « TuttiOperation.csv » devrait être retirée a posteriori, il est possible de le faire sous l’entête “Stations de références”.<br>

```{r, echo=FALSE, warning=FALSE}

tutti_operation <- readRDS("data/operation.rds")
map_station <- tutti_operation %>% dplyr::filter(Annee == 1999)
polygon <- readRDS("data/polygon.rds")

#leaflet
carte <-
  leaflet() %>% addTiles() %>% addPolygons(
    data = polygon,
    opacity = 1,
    dashArray = "5,10",
    label = "Zone de la concession",
    labelOptions = labelOptions(textsize = "15px"),
    weight = 3,
    fillOpacity = 0.1,
    color = "black"
  )

for (i in 1:nrow(map_station)) {
  if (grepl("H", map_station$Code_Station[i])) {
    carte <-
      carte %>% addPolylines(
        lng = c(map_station$LongDeb[i], map_station$LongFin[i]) ,
        lat = c(map_station$LatDeb[i], map_station$LatFin[i]),
        label =  paste(
          map_station$Code_Station[i],
          ": Zone",
          map_station$zones[i],
          "Station impactée du",
          as.Date(map_station$impact_date_1[i]),
          "au",
          map_station$impact_date_2[i]
        ),
        labelOptions = labelOptions(textsize = "15px"),
        color = brewer.pal(n = 9, name = "Reds"),
        opacity = 0.8
      )
  } else {
    carte <-
      carte %>% addPolylines(
        lng = c(map_station$LongDeb[i], map_station$LongFin[i]) ,
        lat = c(map_station$LatDeb[i], map_station$LatFin[i]),
        label = paste(map_station$Code_Station[i], ": Station de référence"),
        labelOptions = labelOptions(textsize = "15px"),
        color = brewer.pal(n = 9, name = "Blues"),
        opacity = 0.8
      )
  }
}
carte <- carte %>% setView(lng = map_station["LongDeb"][1,1],
                             lat = map_station["LatDeb"][1,1], zoom = 11)
carte
```

**- Cas n°3 : vous avez déjà utilisé l’outil pour traiter vos données. Vous possédez un fichier récapitulatif de l’ensemble des paramètres utilisés pour une précédente analyse et vous souhaitez repartir de ce fichier** <br> Dans le cas où vous avez déjà sauvegardé le paramétrage dans un fichier, vous pouvez l’importer à la suite des fichiers «TuttiCatch.csv » et « TuttiOperation.csv » ; ainsi les champs concernant les stations seront remplis automatiquement. <br>

### Production des tables d'indicateurs

Lorsque vous avez complété l'étape de chargement des données, vous pouvez appuyer sur le bouton avec le dragon vert. Cela lancera, en interne de l'application, le calcul des différents indicateurs et covariables nécessaires à l'analyse des données. Si vous n'appuyez pas sur ce bouton, rien ne se passera et vous ne serez pas en mesure de poursuivre l'analyse. <br>

**Nota Bene :** Si vous avez plus d'une concession à analyser, vous pouvez revenir dans cet onglet, changer les fichiers en chargant ceux correspondant à cette autre concession (« TuttiCatch.csv », « TuttiOperation.csv », « ShapeFiles »), puis appuyer à nouveau sur le dragon vert pour relancer la production des tableaux d'indicateurs.

### Tableau général

Dans l'onglet "Tables", il y a un tableau de données à droite et une partie interactive à gauche. Le tableau affiché est formé à partir des données renseignées dans l'onglet "Mise en forme des données". Les fonctions de mises en forme du tableau vont calculer l'abondance, la biomasse et différents indicateurs de diversité pour chaque station et pour chaque campagne. La variable "traitement", indicatrice de l'état de chaque station peut prendre deux valeurs "sans impact" ou "impact". Elle renseigne si la station est à l'intérieur du périmètre de la concession et donc considérée comme étant impactée par les travaux d'extraction de granulats (i.e. modalité "impact") ou si la station est en dehors du périmètre de la concession (i.e. modalité "sans impact"). Dans le cas d'un état initial, où il n'y a pas eu d'extraction sur le site de la concession étudiée, les stations situées à l'intérieur de la concession se voient attribuer l'état "sans impact" jusqu'à la date de début d'exploitation. Cela permet de les considérer comme reflétant l'état de l'environnement avant tout impact de l'extraction dans le cadre de l'analyse statistique déployée ensuite.

```{r, echo=FALSE, warning=FALSE}

dataset <- readRDS("data/complete_dataset.rds")
dataset$saison <- factor(dataset$saison, levels = c("Winter","Spring", "Summer", "Autumn"))
dataset <- dataset[, 2:length(dataset)]

# Define the callback function to add custom HTML and CSS
callback <- JS(
  '$(document).ready(function() {',
  '  var container = $(".dataTables_wrapper");',
  '  container.css("overflow-x", "auto");',  # Enable horizontal scrolling
  '});'
)

# Create the datatable with the callback function
datatable(dataset, callback = callback)

```

Lors des processus de mise en forme, la modalité saison est calculée à partir des dates de début d'échantillonnage. C'est le cadre administratif qui est choisi par défaut pour déterminer les saisons. Néanmoins, il est possible de changer cette colonne dans le tableau complet. Une attention particulière est portée à la notion de saison car cela fait partie intégrante de l'évaluation de la variabilité temporelle des communautés halieutiques (paragraphe 8.1.4 du Protocole halieutique). Selon le "Protocole halieutique", les effets de la variabilité saisonnière sur les assemblages (groupe d’espèces) halieutiques dépendent beaucoup de la latitude. Dans les eaux du Nord (mer du Nord, Manche, nord du golfe de Gascogne), il est courant de n’observer que deux types d’assemblages halieutiques par an, un assemblage d’hiver pendant environ huit mois de l’année et un assemblage d’été pendant environ quatre mois. Dans les eaux plus chaudes du Sud (sud du golfe de Gascogne, Méditerranée), les assemblages saisonniers sont potentiellement plus nombreux avec des assemblages de printemps et d’automne plus marqués. Néanmoins, c'est la conduite de l'état initial qui permettra de déterminer la variabilité saisonnière localement et de décider de la périodicité saisonnière des campagnes de suivi. En modifiant la colonne "saison" du tableau, il est ainsi possible de s'ajuster aux conditions locales et aux difficultés d'échantillonnages.

Vous pouvez modifier l'affichage général du tableau de données à l'aide de la flèche située sous le message "quel tableau afficher" et il est possible de télécharger le tableau affiché via le bouton "Telecharger la table". Le bouton "Telecharger les informations rentrées" permet d'enregistrer dans un fichier csv la liste des stations d'impact, les dates d'exploitation et la largeur d'ouverture du chalut utilisées dans l'onglet "Importation des données". A l'issue de cet onglet, vous pouvez décider de réaliser la partie "statistique exploratoire" qui s'intéresse à la communauté dans sa globalité, ou de passer directement à la partie "statistique descriptive" qui se concentre sur une variable spécifique.

## <i class="fa-solid fa-eye"></i> Statistiques exploratoires

### Représentation des indicateurs

Dans cette partie on s'intéresse aux indicateurs de biodiversité et d'abondance obtenues à l'échelle de la communauté, à l'intérieur et à l'extérieur de la concession, pour chaque campagne de collecte de données réalisée. Les indicateurs présentés sont ceux référencés dans le "Protocole halieutique" article 8.4.1.

####   Définition des indicateurs

La biodiversité englobe la variété de la vie, à tous les niveaux d'organisation, classée selon des critères évolutifs (phylogénétiques) et écologiques (fonctionnels). Au niveau des populations biologiques, la variation génétique entre les organismes individuels et entre les lignées contribue à la biodiversité en tant que signature de l'histoire évolutive et écologique et base de l'évolution adaptative future. C'est au niveau des espèces que le terme de biodiversité est le plus souvent appliqué par les écologistes et les biologistes de la conservation. La richesse spécifique fait référence au nombre total d'espèces présentes dans un écosystème donné. C'est une mesure simple qui ne prend en compte que le nombre d'espèces sans considérer leur abondance relative. Par exemple, si une forêt tropicale contient 100 espèces d'arbres différents, sa richesse spécifique serait de 100.

Un indice de diversité est une expression mathématique qui combine la richesse et la régularité des espèces pour mesurer la diversité. L'objectif principal d'un indice de diversité est d'obtenir une estimation quantitative de la variabilité biologique qui peut être utilisée pour comparer des entités biologiques dans l'espace ou dans le temps. Cet indice prend en compte deux aspects différents qui contribuent au concept de diversité dans une communauté : la richesse en espèces et l'homogénéité.

L'indice de diversité de Shannon-Weaver est un indice largement utilisé pour comparer la diversité entre différents habitats. Il suppose que les individus sont échantillonnés au hasard dans une grande population indépendante et que toutes les espèces sont représentées dans l'échantillon. Cet indice mesure à la fois la richesse spécifique et l'équité (ou l'uniformité) de la distribution des espèces dans un écosystème. Il prend en compte à la fois le nombre d'espèces présentes et leur abondance relative. Plus précisément, l'indice de Shannon-Weiner est calculé selon la formule suivante :

$$ H′= -\sum_{i}^S (p_{i}*ln(p_{i})) $$

où :<br> **S** est le nombre total d'espèces, <br> **pi** est la proportion de la i-ème espèce parmi toutes les espèces présentes, <br> **ln** représente le logarithme népérien

La valeur de l'indice de diversité de Shannon-Weaver est généralement comprise entre 1,5 et 3,5 et ne dépasse que rarement 4,5. Un indice de Shannon-Weiner plus élevé indique une plus grande diversité d'espèces et une distribution plus uniforme entre ces espèces.

Contrairement à l'indice de Shannon-Weiner, l'indice de Simpson se concentre principalement sur la dominance des espèces les plus abondantes dans un écosystème. Il est calculé selon la formule suivante :

$$ D= \sum_{i}^S (p_{i}*(p_{i}-1)) $$

où les termes sont les mêmes que dans l'indice de Shannon-Weiner. Un indice de Simpson plus élevé indique une biodiversité plus faible, car il met davantage l'accent sur la probabilité qu'une espèce choisie au hasard soit la même que celle choisie précédemment.

####   Représentation dans l'application

Dans un premier temps, le tableau (ci-dessous) affiche les valeurs moyennes d’abondance, de biomasse, de richesse spécifique, des indicateurs de Shannon et de Simpson à l’intérieur de la concession, à l’extérieur de la concession, et au global pour chaque campagne. Ils sont calculés à partir des valeurs obtenues en chaque station échantillonnée. Pour plus de lisibilité, les écarts-types ne sont pas affichés dans le tableau présent dans l'application mais sont disponibles dans le fichier cvs téléchargeable via le bouton "Telecharger le tableau".

```{r, echo=FALSE, warning=FALSE}
source("R/fct_diversity_table.R")

ID_campagne <- c()
for (i in (1:max(dataset['campagne']))){
  ID_campagne <- c(ID_campagne, paste("C",i,sep=""))
}


data_indic <- cbind(
    "ID_campagne" = ID_campagne,
  diversity_table(dataset, "Abun"),
  diversity_table(dataset, "Biom" ),
  diversity_table(dataset, "Richness" ),
  diversity_table(dataset, "Shannon" ),
  diversity_table(dataset, "Simpson" )
)

name_indic <- names(data_indic)
position <- grep("mean", name_indic)
show_data_indic <- data_indic[,position]
show_data_indic <- data.frame("ID_campagne" = data_indic[,1], show_data_indic)

datatable(show_data_indic, callback = callback)
```

Les graphiques proposés ci-dessous représentent les valeurs moyennes (points) et les percentiles 5 et 95 (barres hautes et basses) obtenus pour les mêmes indicateurs que ceux du tableau en fonction de la campagne sélectionnée, et selon le secteur échantillonné (paragraphe 8.4.1 du Protocole halieutique). Ils permettent de visualiser rapidement les différences de valeurs obtenues entre la zone de concession et la zone de référence pour les indicateurs de biodiversité les plus courants.

```{r, echo=FALSE, warning=FALSE, fig.height = 9, fig.width = 16}
source("R/fct_lineplot_creation.R")
campagne <- "C1"
abun_plot <- lineplot_creation(data_indic, "Abun", campagne)
biom_plot <- lineplot_creation(data_indic, "Biom", campagne)
richness_plot <- lineplot_creation(data_indic, "Richness", campagne)
shannon_plot <- lineplot_creation(data_indic, "Shannon", campagne)
simpson_plot <- lineplot_creation(data_indic, "Simpson", campagne)
frise <- plot_grid(abun_plot, biom_plot, richness_plot,
                       shannon_plot, simpson_plot)
frise
```

<br> L'intérêt de ces approches est de pouvoir comparer la communauté halieutique à plusieurs échelles. Dans un premier temps, la comparaison est axée sur intérieur ou extérieur de la concession. Mais si les campagnes sont regardées les unes après les autres, il pourrait être possible de distinguer des changements au cours du temps. Il y a à la fois un aspect spatial et temporel.

### Représentation de la structure

Ce tableau représente la proportion de chaque espèce présente pour chaque campagne d'échantillonnage (paragraphe 8.4.1 du Protocole halieutique). Le tableau permet de suivre l'évolution des proportions d'espèces au fil du temps et offre une perspective sur les tendances des assemblages. Les variations des proportions des différentes espèces d'une année à l'autre peuvent renseigner des changements écologiques significatifs, tels que des fluctuations dans la biodiversité, des modifications des habitats ou des pressions environnementales. Ce tableau peut permettre aussi d’identifier les espèces qui dominent dans un écosystème donné ainsi que celles qui sont en déclin.

```{r, echo=FALSE, warning=FALSE}
source("R/fct_structure_table.R")

catch <- readRDS("data/catch.rds")
species <- unique(catch['Nom_Scientifique'])[,1]

data_brut <- data.frame(ID_campagne)
for (sp in species){
  data_brut <- cbind(data_brut, structure_table(dataset, sp))
}
position <- grep("tot_value", names(data_brut))
data_brut <- data_brut[,c(position)] 
tot <- rowSums(data_brut)
data_brut <- cbind(ID_campagne, data_brut, tot)
names(data_brut) <- c("ID_campagne",species,"Total")



for (i in 1:nrow(data_brut)){
   for (sp in 1:length(species)){
     data_brut[i,sp+1] <- round(data_brut[i,sp+1]/data_brut$Total[i], digits = 2)
   }
}
data_percent <- t(data_brut[,2:(length(species)+1)])
#redefine row and column names
colnames(data_percent) <- ID_campagne
data_percent <- as.data.frame(cbind(species,data_percent))
names(data_percent)[1] <- c("species")
datatable(data_percent, callback = callback)

```

La figure ci-dessous est composé de trois graphiques représentant l'abondance des espèces dans une campagne donnée par ordre décroissant (à gauche), la contribution relative de chaque espèce à l'abondance totale (en haut à droite) et la courbe d'accumulation des espèces (en bas à droite). Elle apporte des éléments pour répondre aux attentes du paragraphe 8.4.1 du "Protocole halieutique".

```{r, echo=FALSE, warning=FALSE, fig.height = 9, fig.width = 16}
#### prepare data ####
      data_t <- as.data.frame(t(data_brut[,2:(length(species)+1)]))
      #rownames(data_t) <- species
      colnames(data_t) <- ID_campagne
      data_t <- cbind(species,data_t)
      names(data_t) <- c("species", names(data_t)[2:length(names(data_t))])

      # order the data to have the most present species
      data <- as.data.frame(data_t[c("species","C1")])
      names(data) <- c("species", "Abundance")
      #data <- data %>% dplyr::arrange(desc(Abundance))
      data <- data[order(data$Abundance, decreasing = TRUE), ]
      data$species <- factor(data$species,data$species)

      #### barplot ####
      # Default bar plot
      barplot <- ggplot(data, aes(x=species, y=Abundance, fill=species)) +
        geom_bar(stat="identity", color="black",
                 position=position_dodge())+
      # Finished bar plot
      labs(title=paste("Abundance per species for the survey ",
                         "C1", sep=""),
           fill = "")+
        theme_classic() +
        # Reduce the size of the plot
        theme(
          legend.position = "bottom",
          legend.box = "horizontal",
          legend.margin = margin(t = 0, unit = "cm"), # Adjust margin if needed
          legend.text=element_text(size=8))+ # Adjust legend text size if needed
        scale_fill_viridis_d()
      # change the display of species for too many species
      if (length(data$species) > 5){
        # Set the threshold for displaying species names
        threshold <- 5  # Adjust this value based on your preference

        # Get the species names and filter them based on the threshold
        species_names <- data$species
        filtered_species_names <-
          ifelse(seq_along(species_names) %% threshold == 0, species_names, "")

        # Update x-axis labels with filtered species names
        barplot <- barplot + scale_x_discrete(labels = filtered_species_names)}
      
      #### cumulative plot ####
      # Create a cumulative sum of abundances
      data$cumulative_abundance <- cumsum(data$Abundance)

      # Create the cumulative abundance curve with number of species
      # on the x-axis using ggplot2 and geom_step
      cumul_plot <- ggplot(data, aes(x = seq_along(cumulative_abundance),
                                          y = cumulative_abundance)) +
        geom_step() +
        labs(title = "Cumulative Abundance Curve",
             x = "Number of Species",
             y = "Cumulative Abundance")
      
       #### Species accumulation curves ####
      indice_campagne <- 1
      SAC <- specaccum(dataset %>%
                         dplyr::filter(campagne==indice_campagne) %>%
                         dplyr::select(species), "random")
      table_SAC <- data.frame(site = SAC$sites, richness = SAC$richness,
                              sd = SAC$sd)# table_SAC export the information
      #from the list generate by speccacum
      SAC_plot <- ggplot(table_SAC, aes(site))+
        geom_ribbon(aes(ymin = richness - sd,
                        ymax = richness + sd), fill = "lightblue")+
        geom_line(aes(y=richness), color="blue")+
        #geom_errorbar(aes(ymin=richness-sd, ymax=richness+sd), width=.2,
        #              position=position_dodge(0.5)) +
        labs(title = "Species accumulation curves",
             x = "Number of Sites",
             y = "Number of Species")

      ### prepare the plot to be display ####
      p <- plot_grid(cumul_plot, SAC_plot, ncol = 1, rel_heights = c(5,5))

      plot_grid(barplot,p, ncol = 2, rel_widths = c(5,4))
```

La figure de gauche représente l'abondance de chaque espèce pour une campagne dans un histogramme ordonné par ordre d’abondance décroissante. Cela permet d'identifier les espèces dominantes au sein d'une campagne, c'est-à-dire celles qui sont les plus abondantes dans l'échantillon. Ces espèces dominantes peuvent jouer un rôle crucial dans la structuration de l'écosystème étudié, influençant par exemple la compétition pour les ressources ou la prédation sur d'autres espèces. <br> De plus, en observant les changements d'une campagne à une autre, cet histogramme peut aider à visualiser les tendances temporelles. Par exemple, une augmentation ou une diminution de l'abondance d'une espèce dominante pourrait indiquer des changements dans les conditions environnementales ou dans les interactions entre les espèces. De même, il permet de détecter les variations saisonnières et annuelles, telles que les pics de reproduction ou les migrations saisonnières qui peuvent influencer la composition de la communauté. <br> Comparer les données entre différentes campagnes est également facilité par cet histogramme. En mettant côte à côte les distributions d'abondance des espèces pour différentes périodes, cela peut permettre d'identifier les similitudes et les différences entre les écosystèmes à différents moments de l'année. Cette comparaison peut révéler des schémas écologiques généraux ou des réponses spécifiques à des perturbations environnementales. <br> Pour plus de lisibilité, les espèces participant à moins de 1% de l’abondance totale ne sont pas affichées sur l’histogramme.

La figure en haut à droite est une courbe d'abondance cumulée en fonction du nombre d'espèces par ordre d'abondance décroissante. L'abondance cumulée se réfère à la somme cumulative des abondances des espèces dans un ensemble de données, en commençant par l'espèce la plus abondante et en ajoutant successivement les abondances des espèces suivantes dans l'ordre décroissant. La courbe d'abondance cumulée permet d'évaluer la diversité et la répartition des espèces dans un écosystème ou dans un échantillon biologique. Plus la courbe est plate, plus la communauté est diversifiée alors qu’une courbe qui monte rapidement puis s'aplatit indique une communauté où quelques espèces sont très abondantes tandis que la plupart des espèces sont rares. Attention la courbe d'abondance cumulée apporte un intérêt lorsqu'il y a de nombreuses espèces différentes. Ici, la courbe est construite à partir du jeu de données d'une concession fictive avec seulement 4 espèces. Elle présente donc un intérêt limité. En pratique, vous ne devriez pas avoir ce genre de résultats avec vos données.

La figure en bas à droite représente le nombre d'espèces en fonction du nombre de sites échantillonnés. L'algorithme va, pour un nombre de sites donnés, tester toutes les combinaisons existantes dans le jeu de données et récupérer le nombre d'espèces pour chaque combinaison. Ensuite il fait une moyenne par nombre de site et c'est cette valeur moyenne qui est reportée dans le graphique. <br> La forme de la courbe d'accumulation des espèces peut fournir des informations sur la diversité de l'écosystème étudié. Si la courbe augmente rapidement et tend vers une asymptote plate, cela suggère que la plupart des espèces présentes ont été échantillonnées et que l'on a une bonne estimation de la diversité de l'écosystème. En revanche, si la courbe augmente lentement et ne semble pas atteindre un plateau, cela indique qu'il reste encore des espèces à découvrir et que l'échantillonnage doit être poursuivi pour obtenir une estimation plus précise de la diversité.<br> La courbe d'accumulation des espèces est utile pour déterminer le nombre minimum d'échantillons nécessaires pour obtenir une représentation adéquate de la diversité des espèces. Cela peut être utiliser lors des deux années d'état initial pour s'assurer que le plan d'échantillonnage proposé permet de capter la diversité de la zone. Ici, construite à partir du jeu de données factice, la courbe ne présente que peu d'intérêt.

## <i class="fa-solid fa-scroll"></i> Statistiques descriptives

### Représentation des données

Dans cette partie on s’intéresse à un indicateur en particulier (abondance d’une espèce, biomasse totale, indicateur de diversité, ...) et on le compare aux variables explicatives de notre jeu de données. On recherche des effets ou des corrélations possibles en amont des statistiques inférentielles. Il faut donc choisir une variable que l’on cherche à expliquer en fonction de paramètres liés à l’acquisition des données et à l’extraction. C’est sur cette variable que sera effectuée l’analyse statistique. Dans un premier temps le tableau qui résume la variable expliquée renseigne sur le nombre de zéros et valeurs manquantes, la longueur totale de la série de valeurs et la fraction du nombre de zéros et valeurs manquantes sur le total de valeurs. Ensuite, il fournit également la moyenne, les extrêmes, l’écart-type et les quartiles de la série. Les valeurs les plus importantes pour la partie "statistiques inférentielles" sont celles renseignant sur le nombre de zéros et valeurs manquantes (“n_missing”, “complete_rate”). La proportion de zéros dans les données structure la méthode de modélisation qui va être employée. En effet, une trop grande proportion de zéros peut compromettre la réalisation d’un modèle linéaire mixte généralisé.  <br>

```{r, echo=FALSE, warning=FALSE}
source("R/fct_numeric_summary.R")

dataset$year <- as.factor(dataset$year)
dataset$campagne <- as.factor(dataset$campagne)

data <- dataset$Abun
datatable(numeric_summary(as.numeric(data), "Abun"),
         callback = callback) 

```

A la suite de ce tableau, il est possible de visualiser des boites à moustaches. La boite à moustache ou boxplot offre une autre représentation pour interpréter le lien entre la variable expliquée et les variables explicatives comme l’impact, l’année, la campagne, la station et la saison. Dans les représentations graphiques de données statistiques, le boxplot est un moyen rapide de figurer le profil essentiel d’une série statistique quantitative. Le boxplot résume quelques indicateurs de position du caractère étudié (médiane, quartiles, minimum, maximum ou déciles). Il est souvent utilisé pour comparer rapidement deux séries. Dans GranulatShiny, la série de la variable expliquée (ici l’abondance) dans la zone avec impact est comparé avec celle de la zone sans impact. Dans la figure ci-dessous, la même variable est représentée en échelle décimale (à gauche) et en échelle logarithmique (à droite). L’échelle logarithmique est proposée dans l’application pour ramener la variable dans une distribution pseudo-normale et donner plus de sens à la représentation en boxplot en limitant l’influence des valeurs extrêmes. Chaque boxplot est construit de la manière suivante : le trait horizontal qui traverse le carré blanc correspond à la médiane, les bords haut et bas du carré blanc correspondent au 75eme et au 25eme quartile respectivement, les extrémités des “moustaches” correspondent au 95eme et au 5eme percentile ; enfin, les points correspondent aux valeurs extrêmes.

```{r, echo=FALSE, warning=FALSE}

b1 <- ggplot(dataset, aes(x = traitement, y = Abun))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "Abun"," by impact", sep=""),
                x = "", y = "")

b2 <- ggplot(dataset, aes(x = traitement, y = log(Abun+1)))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "log(Abun)"," by impact", sep=""),
                x = "", y = "")
plot_grid(b1,b2)

```

Dans le paragraphe 8.4.1 du "Protocole halieutique", il est indiqué de décrire et analyser les données par groupe de taille, par maturité ou par groupe fonctionnel. L'application GranulatShiny ne permet pas de redécouper les tableaux données en sous-groupe. Cela est à faire en amont par l'utilisateur. Pour analyser un groupe fonctionnel particulier, il faut faire un tri sur son fichier "TuttiCatch" et enregistrer ce nouveau tableau pour pouvoir l'intégrer dans l'application. Cela permet d'appliquer la méthode statistique de GranulatShiny sur les espèces à traiter séparément.

Après l'exploration des données, il est possible de passer à l'onglet suivant en appuyant sur le bouton "Choisir la probabilité de distribution" ou en cliquant sur “diagnostic d’analyse”.

### Diagnostic d'analyse

Cet onglet permet de choisir et de visualiser la distribution de probabilité qui correspond le mieux à la variable expliquée. L'histogramme de fréquence de la variable à l'étude (sélectionnée par l'utilisateur) est représenté en bâtons gris. Il représente la distribution empirique des données observées. Il est construit en regroupant les données de la variable en intervalles et en comptant le nombre d'observations dans chaque intervalle. Cela donne une visualisation de la répartition des valeurs de la variable.<br> La fonction de densité (en bleu dans l'exemple) est une estimation de la distribution de probabilité des données de la variable. Elle est calculée en ajustant différents modèles de distribution statistique aux données observées. Ces modèles peuvent inclure des distributions normales, de Poisson, exponentielles, etc. La fonction de densité représente la probabilité qu'une observation tombe dans une plage particulière de valeurs.<br> La distribution de probabilité (en vert dans l'exemple) représente le modèle de distribution statistique qui correspondrait le mieux à la fonction de densité. Elle est choisie par l'utilisateur pour se superposer au mieux à la courbe bleue. Les paramètres de chaque distribution de probabilité sont approximés à l'aide de la moyenne et de l’écart type de la variable.

En examinant ces trois éléments ensemble, vous pouvez évaluer visuellement à quel point le modèle de distribution de probabilité ajusté correspond aux données observées. Une bonne correspondance entre les trois indique que le modèle est une représentation précise de la distribution des données. Cependant, des écarts importants peuvent indiquer des inadéquations dans le modèle choisi ou des caractéristiques particulières des données qui nécessitent une analyse plus approfondie. Vous pouvez changer le type de distribution de probabilité afin de tester celle qui semble correspondre le mieux. Si la distribution choisie ne correspond pas du tout, un message d'avertissement apparaît. Dans l'exemple c'est l'abondance qui est représenté et la loi choisie est une loi Lognormale.<br>

```{r, echo=FALSE, warning=FALSE}
source("R/fct_probability_distribution.R")

ggplot(data = dataset, aes(x = Abun, y = after_stat(density))) +
        geom_histogram(colour = "black", fill = "grey", bins = 100) +
        geom_density(alpha = .3, fill = "blue") +
        geom_area(aes( y= probability_distribution(dataset$Abun, "Lognormale")),
                  color="darkslategray", fill = "darkseagreen",
                  alpha = 0.4, linewidth = 1)+
        xlab("Abun") +
        theme_minimal()
```

Lorsque vous êtes satisfait de la distribution des probabilités, vérifiez la phrase au-dessus du bouton "Passer à la modélisation". Il y a deux possibilités. Dans le cas où vous avez moins de 30 observations, la phrase dit : "Vous n'avez pas assez de valeurs pour passer à la partie modélisation". Dans ce cas, vous devez changer la variable de travail car il n'y a pas assez de valeur pour créer un modèle pertinent. A l'inverse, si le volume de données présent dans le jeu de données est suffisant (\>30 observations), vous aurez : "Après avoir choisi une distribution de probabilité, vous pouvez passer à la construction du modèle". Lorsque vous avez terminé, appuyez sur le bouton "passer à la modélisation".

## <i class="fa-solid fa-fish-fins"></i> Modélisation

### Création des modèles

Cette partie est consacrée à la création d'un modèle pour l'analyse inférentielle. La variable analysée est rappelée en haut à gauche de l'onglet et peut être modifiée dans l'onglet "représentation des données". Dans l'application il est possible de réaliser 3 types de tests inférentiels : GLMM, GLM, PERMANOVA. Dans un premier temps ce chapitre rappelle les principes généraux de statistique employés dans l'application.<br>

####   Principes généraux de statistiques nécessaires à l’utilisation de l’outil

#####     [Méthode paramétrique et non-paramétrique]{.underline}

Le domaine des statistiques existe parce qu'il est impossible de collecter des données auprès de tous les individus concernés (population). La seule solution consiste à collecter des données auprès d'un sous-ensemble (échantillon) des individus concernés, mais le véritable objectif est de connaître la "vérité" sur la population. La population est approchée en étudiant des variables descriptives. Chaque variable est un objet statistique, qui peut être décrit par des indicateurs. Les indicateurs statistiques tels que la moyenne, l'écart-type et les quartiles servent à résumer l'information concernant une variable observée. Lorsque l'on étudie un échantillon considéré comme représentatif, ces indicateurs servent à construire la loi de distribution de la variable étudiée. Chaque indicateur correspond à un "paramètre" de cette loi de distribution. On considère alors que la loi de distribution obtenue pour cette variable à partir de l'échantillon est applicable à la population. Étant donné que l'on ne peut généralement pas obtenir de données sur l'ensemble de la population, on ne peut pas connaître les valeurs des paramètres pour cette population. Il est toutefois possible de calculer des estimations de ces quantités pour l'échantillon. Lorsqu'elles sont calculées à partir des données de l'échantillon, ces quantités sont appelées "statistiques". Une statistique estime un paramètre. Les procédures statistiques paramétriques reposent sur des hypothèses concernant la forme de la distribution (c'est-à-dire une distribution normale) dans la population sous-jacente et sur la forme ou les paramètres (c'est-à-dire les moyennes et les écarts-types) de la distribution supposée. Les procédures statistiques non paramétriques ne reposent sur aucune ou peu d'hypothèses concernant la forme ou les paramètres de la distribution de la variable dont l'échantillon a été tiré.

#####     [Modèles linéaires]{.underline}

Un modèle linéaire classique est une méthode paramétrique qui permet d’étudier la liaison statistique entre une variable réponse **Y** et les variables explicatives **X**. Soit yi la réponse de l’individu **i** et **x<sub>i</sub>** les valeurs prises par les variables explicatives pour cet individu. La relation entre **X** et **Y** peut s’écrire sous la forme : $$Y = α + \sum_jβ_jX_j + ε $$ où **ε** représente les résidus du modèle, la variance de la variable **Y** non expliquée par les variables explicatives **X**, distribuée selon une loi normale d’espérance nulle. Le terme **α** correspond à ce qu’on appelle l’intercept et **β<sub>j</sub>** représente les coefficients estimés du modèle des variables explicatives **X<sub>j</sub>**. La variable réponse pour un modèle linéaire doit être une variable approximativement normalement distribuée.

#####     [Modèles linéaires généralisés]{.underline}

Les modèles linéaires trouvent une large application, mais ne peuvent pas gérer des réponses continues clairement discrètes ou asymétriques. Par exemple, les variables réponses de type “comptage”, souvent asymétrique ainsi que les variables binaires comme la présence/absence ne suivent pas une loi normale. Les modèles linéaires ne sont donc pas adaptés à ce type de variables. Les modèles linéaires généralisés (GLM) permettent l'extension des idées de modélisation linéaire à une classe plus large de types de réponse, comme celles énoncées précédemment, sous une méthodologie de modélisation commune. Une chose importante à comprendre dans les GLM est la relation entre les valeurs de la variable de réponse, **Y** (telles que mesurées dans les données et prédites par le modèle dans les valeurs ajustées) et le prédicteur linéaire. Le prédicteur linéaire émerge du modèle linéaire comme une somme de chaque terme du modèle. Le prédicteur linéaire correspond à la variable **Y** seulement lors d’un modèle linaire classique suivant une loi normale. Dans le cas de modèle linéaire généralisé, c’est la fonction de lien, **g**, qui relie la valeur **Y** à son prédicteur linéaire **N**. $$ N =g(Y) $$ La valeur de **N** est obtenue en transformant la valeur de **Y** par la fonction de liaison **g**, et la valeur prédite de **Y** est obtenue en appliquant la fonction de liaison inverse à **N**. <br> En utilisant différentes lois de distribution et donc différentes fonctions de lien, il est possible d’observer les conséquences sur les hypothèses des résidus du modèle. La fonction de lien la **plus appropriée** est celle qui produit **les résidus les plus conformes**.

#####     [Modèle linéaire généralisé mixte]{.underline}

Les modèles linéaires mixtes généralisés (GLMM) sont une extension des GLM. Un GLMM est dit "mixte" parce qu'il comprend au moins un effet "fixe", les variables explicatives et au moins un effet "aléatoire". Les effets aléatoires ne sont pas des termes évalués, ils servent uniquement à indiquer au modèle que les données ne sont pas indépendantes et reflètent une corrélation entre les unités statistiques. D'un point de vue statistique, cela permet d'estimer précisément la déviance résiduelle et donc d'éviter de biaiser l'erreur standard des paramètres. Au final, cela se traduit par des p-values plus fiables.

#####     [Analyse de variance par permutations]{.underline}

La PERMANOVA, ou Analyse de Variance Permutationale Multivariée, est une méthode statistique qui permet d'analyser les différences entre plusieurs groupes définis par des caractéristiques qualitatives, comme par exemple les différents traitements dans une étude expérimentale.

Contrairement à d'autres méthodes statistiques qui nécessitent certaines hypothèses sur la distribution des données, la PERMANOVA ne se base pas sur ces suppositions. Elle se focalise plutôt sur une matrice de distance entre les éléments étudiés. Cette approche lui permet de travailler avec des données de différentes dimensions, qu'elles soient simples ou complexes, et peu importe le nombre de catégories.

L'objectif de la PERMANOVA est de déterminer s'il existe des différences significatives dans la variabilité entre les groupes. Pour ce faire, elle évalue la variation entre les groupes (SS inter) par rapport à la variation à l'intérieur des groupes (SS intra). Une SS inter élevée suggère des différences importantes entre les moyennes des groupes, tandis qu'une SS intra faible indique une similarité accrue des observations au sein de chaque groupe.

La décision de rejeter ou non l'hypothèse nulle (l'absence de différence entre les groupes) se fait en comparant le rapport entre la variation inter-groupe et la variation intra-groupe à une distribution obtenue par permutation des données. Si ce rapport est significativement élevé, cela indique que les différences observées entre les groupes sont probablement réelles.

Cependant, la PERMANOVA présente quelques limites. Elle ne permet pas de déterminer quel groupe spécifique diffère des autres, seulement qu'au moins un groupe est différent. De plus, la présence de valeurs nulles peut biaiser l'estimation de la similarité entre les éléments, ce qui est particulièrement problématique en écologie où un zéro peut signifier l'absence d'une espèce. Cette limitation peut être atténuée en choisissant un coefficient d'association approprié dans le calcul de la matrice de distance.

####   Ecriture du modèle avec GranulatShiny

Selon la méthode de modélisation que vous choisissez, la formulation du modèle diffère. Si l'on prend l'exemple de l'abondance comme variable réponse, le GLMM prendra en compte deux variables explicatives fixes, le traitement et la saison, et leur interaction ainsi que deux variables explicatives aléatoires, la campagne et la station : $$GLMM → Abun \sim traitement * saison + (1\|campagne) + (1\|station)$$ Le GLM prendra en compte uniquement les variables explicatives fixes prises en compte dans le GLMM et leur interaction : $$GLM → Abun \sim traitement * saison$$ La PERMANOVA prendra en compte les mêmes variables explicatives que le GLM : $$PERMANOVA → Abun \sim traitement * saison$$

Les modèles sont centrés sur la variable traitement car le suivi des concessions d'extraction de granulats marins est basé sur la méthode BACI (Before After Control Impact). Par définition, la méthode BACI compare des sites témoins (c'est-à-dire non impactés) et des sites impactés et teste les différences entre l'avant et l'après. Il s'agit d'une méthode couramment utilisée dans la surveillance de l'environnement océanique et une méthode BACI bien conçue reste l'une des meilleures approches pour les programmes de surveillance des effets sur l'environnement. Malheureusement, cette méthode présente plusieurs limites qui compromettent sa capacité à détecter des effets notamment parce que l’océan est spatialement et temporellement dynamique, et que trouver deux emplacements statistiquement identiques l’un à l’autre tout en étant suffisamment éloignés géographiquement pour être statistiquement indépendants constitue un véritable défi. <br>

Pour un GLMM et un GLM, vous devrez choisir une distribution de probabilité. Par défaut, il propose la dernière distribution de probabilité que vous avez vérifiée dans la partie précédente. Attention la méthode utilisée pour la modélisation est itérative, il se peut donc que la distribution qui semblait la plus adéquate dans la partie précédente n'est pas forcément celle qui permettra de mieux faire converger le modèle. Néanmoins l'onglet "diagnostic d'analyse" devrait avoir permis de sélectionner un nombre de distribution possible pour ne pas avoir à toutes les tester ici.<br>

Vous pouvez également conserver ou non l'interaction entre les covariables traitement et saison. Attention si l'interaction n'apporte rien au modèle celle-ci est retirée automatiquement. Vous pouvez également ajouter d'autres covariables dans votre modèle. Elles seront ajoutées sans interaction avec les autres. Lorsque vous êtes prêt, vous pouvez cliquer sur "démarrer la modélisation".<br>

#####     [Modèle linéaire mixte généralisé]{.underline}

C'est la méthode à prioriser. Dans le paragraphe 8.4.2 du "Protocole halieutique", il est dit que pour évaluer la variabilité temporelle et spatiale des différents indicateurs des ressources halieutiques avant extraction, il faut utiliser des modèles linéaires généralisés à effets mixtes (GLMMs) avec les variables temporelle et spatiale définies comme effets aléatoires croisés plus un effet saisonnier fixe. La première sortie est une reproduction de la sortie du logiciel r pour la ligne de commande correspondante. Vous pouvez choisir d'afficher le tableau d'analyse de la déviance, qui synthétise les résultats de la modélisation en ne fournissant qu'un résumer de l'évaluation de l'importance des effets fixes dans le modèle et de comprendre leur impact sur la variable réponse. Vous pouvez également afficher le résumé exhaustif de ces résultats. Vous pouvez choisir d'afficher les résultats du modèle avant optimisation via le choix initial ou alors le modèle optimisé via le choix final en bas à gauche.<br>

Reproduction de la sortie R du modèle GLMM sur l'abondance.

```{r, echo=FALSE, warning=FALSE}

GLMM <- glmer(log(Abun) ~ traitement * saison + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
GLMM_summary <- summary(GLMM)
GLMM_summary
```

La partie haute de la fenêtre de résultat rappelle le modèle qui a été utilisé pour calculer les effets.

```{r, echo=FALSE, warning=FALSE}
list(GLMM_summary$methTitle,
c(GLMM_summary$family,GLMM_summary$link),
GLMM_summary$call)
```

Il y a le type de modèle avec la méthode de calcul utilisée. Après, il y a la loi de probabilité et sa fonction de lien. Enfin, la commande complete est affichée ce qui permet de vérifier que la bonne commande a été effectuée.

Ensuite, il est possible de lire des scores liés à la vraisemblance du modèle par rapport aux données et aux paramètres sélectionnés pour la construction du modèle.

Etant donné un échantillon observé **(x<sub>1</sub>,...,x<sub>n</sub>)** et une loi de probabilité **P<sub>θ</sub>**, la vraisemblance quantifie la probabilité que les observations proviennent effectivement d'un échantillon (théorique) de la loi **P<sub>θ</sub>**. On appelle vraisemblance associé à la loi de probabilité **P<sub>θ</sub>**, la fonction **L** tel que : $$\displaystyle L(x_1,\ldots,x_n,\theta) = \prod_{i=1}^n P_\theta(x_i)\; $$

```{r, echo=FALSE, warning=FALSE}
GLMM_summary$AICtab
```

Ces indicateurs sont souvent fournis pour évaluer la qualité de l'ajustement du modèle et aider à la sélection du meilleur modèle parmi plusieurs candidats. <br>
**AIC (Critère d'information d'Akaike) :**
L'AIC est un critère de sélection de modèle qui prend en compte à la fois la qualité de l'ajustement du modèle et sa complexité. Il favorise les modèles qui s'ajustent bien aux données tout en étant simples. Un modèle avec un AIC plus bas est considéré comme préférable. Cependant, l'AIC ne fournit pas d'indication sur l'ajustement absolu du modèle, mais seulement sur son ajustement relatif par rapport aux autres modèles candidats.<br>
**BIC (Critère d'information bayésien) :**
Le BIC est un autre critère de sélection de modèle qui, comme l'AIC, prend en compte à la fois l'ajustement et la complexité du modèle. Cependant, le BIC pénalise plus sévèrement la complexité du modèle que l'AIC. Un modèle avec un BIC plus bas est considéré comme préférable. Contrairement à l'AIC, le BIC favorise la parcimonie, ce qui signifie qu'il préfère les modèles plus simples.<br>
**logLik (Log-vraisemblance) :**
La log-vraisemblance est une mesure de l'ajustement du modèle aux données. Elle représente la probabilité que les données observées soient générées par le modèle ajusté. Plus la log-vraisemblance est élevée, meilleure est l'ajustement du modèle aux données.<br>
**Deviance :**
La deviance est une mesure de l'ajustement du modèle par rapport à un modèle de référence, souvent un modèle nul. Elle est calculée comme la différence entre la déviance du modèle ajusté et celle du modèle de référence. Une deviance plus faible indique un meilleur ajustement du modèle aux données.<br>
**df.resid (degrés de liberté résiduels) :**
Les degrés de liberté résiduels représentent le nombre de données indépendantes restantes une fois que le modèle a été ajusté. Ils sont utilisés pour calculer les statistiques de test et les valeurs p associées.

Les "scaled residuals" sont les résidus du modèle. Des tests sont effectués dessus afin de vérifier la bonne convergence et le bon ajustement du modèle.

```{r, echo=FALSE, warning=FALSE}
summary(GLMM_summary[["residuals"]])
```

Le tableau des effets aléaoires est spécifique au GLMM. Il renseigne les informations sur cette partie de la formule :<br> (1 \| campagne) + (1 \| station)

```{r, echo=FALSE, warning=FALSE}
GLMM_summary[["varcor"]]
```

Enfin, il y a la partie sur les effets fixes. Cette partie permet de dresser un diagnostic sur les facteurs et la variable d'étude. Le tableau des effets fixes fournit des informations clés sur les effets estimés des variables prédictives, leur précision et leur importance, aidant ainsi à comprendre les relations entre les variables et à tirer des conclusions sur les données.

**Estimation (Estimate):** Cette colonne indique les coefficients estimés (ou effets) de chaque variable prédictive du modèle. L'effet estimé de l'"Intercept" représente la valeur moyenne estimée de la variable réponse lorsque toutes les autres variables prédictives sont nulles.<br>
**Erreur standard (Std. Error):** Cette colonne indique les erreurs standard associées à chaque estimation de coefficient. Les erreurs standard mesurent la variabilité de l'estimation. Des erreurs standard plus faibles indiquent des estimations plus précises.<br>
**Valeur t (t value):** Cette colonne indique la statistique t permettant de tester l'hypothèse nulle selon laquelle le coefficient est égal à zéro. Elle est calculée en divisant l'estimation par son erreur standard. Des valeurs t absolues plus élevées indiquent une preuve plus forte contre l'hypothèse nulle.<br>
**Pr(>|z|):** Cette colonne indique la valeur p associée à la statistique t pour chaque coefficient. Elle indique la probabilité d'observer les données si l'hypothèse nulle (aucun effet) était vraie. Des valeurs p plus faibles suggèrent une preuve plus forte contre l'hypothèse nulle et indiquent que le coefficient est statistiquement significatif.<br>

```{r, echo=FALSE, warning=FALSE}
GLMM_summary[["coefficients"]]
```

La colonne "Estimate" permet de déterminer la valeur moyenne que prend la variable étudiée (dans notre exemple l'abondance totale) par modalité des variables explicatives. Dans notre exemple, l'abondance est expliquée par la variable "traitement" avec 2 modalités (impact, Sans impact) et la variable "saison" avec 4 modalités (winter, spring, summer, autumn). La ligne (Intercept) correspond à une valeur de base. Cette valeur de base est associée à une modalité de chacune de nos variables. Ainsi en hiver et avec une impact le logarithme de l'abondance (car loi Lognormale) vaut en moyenne 10.68.<br>
Pour connaitre la valeur du logarithme de l'abondance en hiver et sans impact, il faut additionner la valeur Intercept à la valeur de l'estimate de la ligne "traitementSans impact" soit 10.68 + 0.12 qui vaut 10.8. Et pour vérifier si le changement est significatif, il suffit de regarder la colonne "Pr(\>\|z\|)" et vérifier si la valeur est inférieure à 0.05. Maintenant, pour obtenir la valeur du logarithme de l'abondance en été et avec impact, il faut additionner la valeur Intercept à la valeur de l'estimate de la ligne "saisonSummer" soit 10.68 + (-0.16) soit 10.52. Enfin, en additionnant la valeur Intercept avec celle des lignes "traitementSans impact", "saisonSummer", "traitementSans impact:saisonSummer", il est possible d'obtenir la valeur du logarithme de l'abondance en été sans impact soit 10.68 + 0.12 + (-0.16) + 1.06 qui donne 11.70. Dans l’exemple, c’est l’interaction entre la saison et le traitement qui apporte des changements significativement différents. Ainsi, si ces deux variables étaient considérées séparément, leur effet sur l’abondance ne serait pas visible. <br> 

Et la dernière section montre les corrélations entre les termes à effet fixe du modèle. Chaque ligne et chaque colonne représentent un terme à effet fixe, et les valeurs du tableau sont les corrélations entre ces termes. Ces corrélations sont calculées sur la base de la matrice de covariance des estimations des effets fixes. Elles indiquent comment les effets estimés des différents facteurs fixes du modèle sont liés les uns aux autres.

Analyse a posteriori du modèle GLMM sur l'abondance.

```{r, echo=FALSE, warning=FALSE}
plot(simulateResiduals(GLMM))
```

Après avoir obtenu les résultats de la modélisation, l'application propose une série de graphiques pour diagnostiquer la qualité de l'ajustement du modèle, en se concentrant notamment sur l'analyse des résidus. Ces graphiques sont générés à l'aide du package DHARMa ("Residual Diagnostics for Hierarchical (Multi-level/Mixed) Regression Models") dans R.

Le graphique de gauche, appelé "QQ plot residual", est une représentation des résidus attendus par rapport aux observations réelles. Dans ce graphique, chaque point représente un résidu calculé par le modèle pour une observation donnée. Idéalement, ces points devraient suivre de près une ligne rouge diagonale, ce qui signifierait que les résidus sont distribués de manière approximativement normale. Si les points s'éloignent de manière significative de cette ligne rouge, cela suggère une mauvaise adéquation du modèle aux données observées.

En plus de la visualisation des résidus, l'outil DHARMa propose trois tests pour évaluer la qualité de l'ajustement du modèle :<br>
**Test de Kolmogorov-Smirnov :**<br>
Ce test d'hypothèse est utilisé pour évaluer si l'échantillon de résidus suit une loi de distribution connue, déterminée par sa fonction de répartition continue. Une déviation significative des résidus par rapport à cette distribution attendue peut indiquer une inadéquation du modèle aux données.<br>
**Un test de Dispersion :**<br>
Ce test compare l'écart-type observé des résidus à celui qui serait attendu en se basant sur la simulation des données. Si la différence est significative, cela peut suggérer une sous- ou sur-dispersion des résidus par rapport aux attentes du modèle. <br>
**Un test de Valeur Aberrante :** <br>
Ce test vise à vérifier si le nombre d'observations dont les résidus se trouvent en dehors de l'enveloppe de simulation est conforme aux attentes du modèle. Une déviation significative de ce nombre peut indiquer la présence de valeurs aberrantes ou une mauvaise adéquation du modèle.
    
Chaque test fournit une mesure de la déviation par rapport aux attentes du modèle avec une p-value associée. Une p-value faible (< 0,05) indique généralement une déviation significative par rapport aux attentes du modèle, tandis qu'une p-value élevée suggère que la déviation observée pourrait être due au hasard et n'est pas statistiquement significative. Si cette déviation est significative, elle est signalée en rouge, indiquant que le test correspondant n'est pas conforme aux attentes du modèle. Ces diagnostics aident à identifier les inadéquations entre le modèle et les données observées et à guider les ajustements nécessaires pour obtenir un modèle plus approprié.

Sur le graphique de droite, des tests sont réalisés sur l'uniformité et l'homogénéité de la variance des groupes évalués dans le modèle. Le test "within-group deviation from uniformity" est un boxplot qui représente la distribution des déviations des résidus au sein de chaque groupe défini par les modalités des facteurs qualitatifs de votre modèle. Chaque groupe est représenté par une boîte, où la médiane est indiquée par une ligne à l'intérieur de la boîte, le premier et le troisième quartile sont représentés par les bords inférieur et supérieur de la boîte, et les moustaches s'étendent jusqu'aux valeurs maximale et minimale. Les points au-delà de cette limite sont considérés comme des valeurs aberrantes. L'objectif de ce test est d'identifier les groupes pour lesquels les résidus présentent des variations importantes par rapport à une distribution uniforme (ceux-ci apparaissent alors en rouge). Des variations importantes peuvent indiquer une inadéquation du modèle pour certains groupes spécifiques. <br> Le deuxième test correspond à un test de Levene. Le test de Levene est utilisé pour évaluer si les variances des résidus diffèrent significativement entre les groupes définis par les modalités des facteurs qualitatifs. Il teste l'hypothèse nulle selon laquelle les variances sont égales entre tous les groupes. Une p-valeur faible (généralement < 0,05) indique une différence significative dans les variances des résidus entre les groupes, suggérant que l'hypothèse d'homogénéité des variances n'est pas valide.

Dans le cas particulier où il y a trop de modalités différentes dû à de multiple covariables (ou que les variables explicatives soient quantitatives), la partie sur l'uniformité et l'homogénéité des groupes est remplacé par une représentation des résidus du modèle en comparaison avec les prédictions du modèle. Si n'y a aucun problème alors la phrase : "No significant problems detected" s'affiche en haut du graphique. Si des déviations des résidus par rapport à une distribution uniforme à travers différentes quantiles sont significatives ou que les déviations quantiles observées sont statistiquement significatives, les tests asociés apparaîssent en rouge, ce qui suggère une inadéquation du modèle pour certains aspects des données. Enfin, les valeurs aberrantes de la simulation (points de données qui se situent en dehors de la plage des valeurs simulées) sont mises en évidence par des étoiles rouges. Ces points doivent être interprétés avec précaution, car nous ne savons pas "à quel point" ces valeurs s'écartent des attentes du modèle. L'important est de vérifier que les tests de vérification ne soient pas significatifs.

<br>Dans le cas de l'exemple, les tests de Kolmogorov-Smirnov, de Valeur aberrante et de dispersion ne sont pas significatifs donc il n'y a pas de problème. Si un de ces tests s'affichaient en rouge, cela indiquerait que le modèle n'est pas optimal et il serait possible alors de chercher un autre modèle qui s'ajusterait mieux. Comme ces modèles se basent sur des données réelles, il est parfois impossible de trouver un modèle parfait. Il faut alors choisir le modèle avec le moins d'avertissement. On peut voir également que le test d'uniformité est validé mais pas celui d'homogénéité. Une fois le modèle validé, vous pouvez changer d'onglet et passer à la visualisation des effets associés au modèle.<br> Si l'on rajoute la covariable année à notre modèle GLMM, cela représente 96 modalités. Le graphique comparant les groupes est remplacé par un graphique comparant les prédictions globales avec les résidus du modèle.<br>

```{r, echo=FALSE, warning=FALSE}
long_glmm <- glmer(log(Abun) ~ traitement * saison + year + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
plot(simulateResiduals(long_glmm))
```

<br>Dans certains cas, la modélisation GLMM ne converge pas. Cela signifie que les données disponibles ne permettent pas à l'algorithme de calcul, associé à la formulation du modèle décidée par l'utilisateur, d'estimer des valeurs de paramètres. Dans ces cas là, un message d'erreur apparait : *"Il y a une erreur lors de la modélisation. Veuillez changer la loi ou le modèle."* Il est possible aussi que le modèle produise des résultats mais dont l'analyse des résidus a posteriori n'est pas satisfaisante. <br>Exemple :

```{r, echo=FALSE, warning=FALSE}
bad_glmm <- glmer(Abun ~ traitement * saison + (1|campagne) + (1|station), data = dataset, family = gaussian(link = identity))
plot(simulateResiduals(bad_glmm))
```

<br> Lorsque la modélisation par GLMM ne converge pas, il faut lui préférer des méthodes de modélisation associées à  des algorithmes de calcul plus sipmles, c'est-à-dire avec moins de paramètres à estimer. L'outil GranulatShiny en propose deux : le GLM et la PERMANOVA.<br> NOTA BENE : lorsque le jeu de données utilisé pour la modélisation est constitué de 30 observations non nulles ou moins, il est préférable de s'en tenir à la méthode la moins coûteuse en termes de calcul, à savoir la PERMANOVA.

#####     [Modèle linéaire généralisé]{.underline}

Reproduction de la sortie R du modèle GLM sur l'abondance

```{r, echo=FALSE, warning=FALSE}

GLM <- glm(log(Abun) ~ traitement*saison, data = dataset, family = gaussian(link = identity))
summary(GLM)
```

Le GLM sort des résultats proches de celui du GLMM et s'analyse de la même manière. Cependant, celui est moins précis. Il ne prend pas en compte les effets aléatoires induits par l'environnement ou la méthode utilisée.

#####     [Analyses de variances par permutations]{.underline}

Méthode de PERMANOVA intégrée dans l'application

Dans le cas de Granulatshiny, la PERMANOVA est appliquée sur une matrice colonne regroupant des la plupart du temps des abondances ou des biomasses. Ainsi les doubles zéros ne peuvent donc pas être pris en compte dans le calcul de ressemblance. Donc les données sont quantitatives et les doubles zéros ne sont pas considéré donc la méthode qui semble la plus approprié est celle du coefficient de Bray-Curtis. C’est le choix qui avait été fait à l’origine. <br> L’indice de dissimilarité de Bray-Curtis, est utilisé en écologie et biologie pour évaluer la dissimilarité entre deux échantillons donnés, en termes d'abondance de taxons présents dans chacun de ces échantillons. Elle est compris entre 0 (les deux échantillons ont la même composition) et 1 (les échantillons sont totalement dissemblables). La dissimilarité de Bray-Curtis est souvent utilisée dans la littérature. Elle est asymétrique et semimétrique. Elle se calcule comme ceci : $$ d_{jk}=\frac{\sum_{i} |x_{ij}-x_{ik}|}{\sum_{i} (x_{ij}+x_{ik})} $$ où *i = colonne; j,k = lignes comparées; x = valeurs d'abondances*

Dans le cas où la matrice d’entrée n’a qu’une seule colonne soit une seule espèce et que dans les valeurs d’entrées, il existe des zéros, il arrive parfois que le dénominateur soit égale à zéro ce qui n’est pas possible et donc créer une erreur dans la matrice de distance. C’est le cas pour tous les indicateurs habituellement utilisés pour des données d'abondances qui pondèrent leur distance en fonction de l’abondance totale dans les sites comparés. Cette méthode ne pouvant s'utiliser dans notre cas, un autre coefficient de calcul de distance a été recherché. Celui-ci ne devait également pas tenir compte des doubles zéros sur des quantitatives.

La méthode retenue est la métrique du chi². Celle-ci donne davantage de poids aux espèces rares qu’aux espèces communes. Son utilisation est recommandée lorsque les espèces rares sont de bons indicateurs de conditions écologiques particulières. Pour appliquer cette méthode, il faut d'abord standardiser les données selon la méthode du chi² comme ceci : $$ x'_{ij}=\frac{x_{ij}}{\sum x_{j} * \sqrt \sum x_{i}} $$ où *i = colonne; j = ligne; x = valeurs d'abondances*

Ensuite, on calcule la matrice de distance en calculant la distance euclidienne sur la matrice de données standardisées. $$d_{jk}=\sqrt \sum_{i} (x_{ij}-x_{ik})² $$ où *i = colonne; j,k = lignes comparées; x = valeurs d'abondances standardisées*

L'inconvénient avec cette méthode, le calcul des effets se produisant sur des données transformées, il est n'est pas possible de quantifier directement l'impact d'un effet sur la variable initiale. On ne peut donc pas dire si un effet est plus ou moins fort sur la donnée initiale car celui-ci s'applique à la donnée transformée. Par contre, si un effet est considéré significatif sur les données transformées alors il l'est également sur les données initiales.

Reproduction de la sortie R de la PERMANOVA sur l'abondance

```{r, echo=FALSE, warning=FALSE}

vector <- decostand(dataset["Abun"],"chi.square", MARGIN = 2)
vector <- data.frame(as.numeric(vector[1,]))
names(vector) <- "Abun"
dist <- vegdist(vector, method = "euclidean")
result <- adonis2(dist~traitement*saison, data = dataset, permutations = 999)
result

```

Dans le tableu de sortie de la PERMANOVA, on retrouve le nombre de permutations et la formule du modèle. Ensuite on retouve plusieurs indicateurs associés à chaque covariable explicative.<br> **Df (degrés de liberté)**: Cette colonne indique les degrés de liberté associés à chaque terme du modèle.<br> **SumOfSqs (Somme des carrés)**: Cette colonne indique la somme des distances au carré entre les observations dans l'espace multivarié.<br> **R2 (R-carré)**: Cette colonne indique la proportion de variance expliquée par chaque terme du modèle. Par exemple, pour "traitement", 8,34 % de la variation des données peut être expliquée par le facteur traitement.<br> **F (statistique F)** : Cette colonne indique la statistique F pour chaque terme, qui vérifie si la variation expliquée par ce terme est significativement plus importante que ce que l'on attendrait du hasard. Des valeurs F plus élevées indiquent des preuves plus solides contre l'hypothèse nulle d'absence d'effet.<br> **Pr(\>F) (valeur p)**: Cette colonne indique la valeur p associée à la statistique F pour chaque terme. Elle indique la probabilité d'observer les données si l'hypothèse nulle d'absence d'effet (c'est-à-dire si toutes les moyennes des groupes sont égales) était vraie. Des valeurs p plus faibles suggèrent une preuve plus forte contre l'hypothèse nulle et indiquent que le terme est un prédicteur significatif de la variation.<br> La PERMANOVA est la méthode à utiliser en dernière. Elle esty moins précise et apporte moins d'informations qu'un GLMM ou qu'un GLM.

Analyse complémentaire a posteriori de la PERMANOVA sur l'abondance.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

p_value <- result["traitement","Pr(>F)"]

legend <- annotate("text", x = 0.65,
                   y = max(log(as.numeric(data)+1)),
                   label = paste("* p = ",p_value, sep = ""),
                   colour = "red", size = 5)

plot <- ggplot(dataset, aes(x = traitement, y = log(Abun+1)))+
  geom_boxplot()+
  labs(title = paste("Boxplot of ", "log(Abun)"," by impact", sep=""),
                x = "", y = "")+ legend
plot
```

<br>Les mêmes boites à moustaches que dans l'onglet Représentation des données sont affichées. Sauf que si la variable de comparaison (dans l'exemple c'est le traitement) a un effet significatif sur la variable expliquée (ici l'abondance) alors la p-value apparait en rouge en haut à gauche du graphique. S'il n'y a pas d'effet détecté pendant la PERMANOVA alors le message "Pas d'effet" apparait en haut à gauche du graphique.

### Représentation des effets

Cet onglet permet de visualiser graphiquement les effets des variables explicatives sur la variable expliquée dans le cas d'un GLMM ou d'un GLM. Dans le cas d'une PERMANOVA, cette section n'est pas sollicitée et la fenêtre graphique sera blanche. Cette partie retransforme les estimates du modèle en l'unité initiale (dans le cas de l'abondance c'est un nombre par km²). Ainsi on peut voir la valeur moyenne de l'abondance en fonction de la saison et du traitement. D'abord vous devez choisir les deux prédicteurs à représenter.<br>

```{r, echo=FALSE, warning=FALSE}
      #choix du terme
      box(
        solidHeader = F,
        status = "danger",
        selectInput( "pred_1",
          label = "Choississez un premier prédicteur",
          choices = c("traitement","saison"),
          multiple = F,
          selected = c("traitement")),
        selectInput("pred_2",
          label = "Choississez un second prédicteur",
          choices = c("traitement","saison"),
          multiple = F,
          selected = c("saison")),
        width = "100%"
        )
```

Si vous avez plusieurs covariables vous devez les fixer afin de pouvoir visualiser le graphique. <br> Dans l'exemple d'un GLM qui regarde l'abundance totale en fonction du traitement et de la saison, voici le graphique obtenu :

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggpredict(GLM, terms = c("traitement", "saison"))|> plot()
```

<br>Ce graphique est une autre manière de représenter le tableau de sortie du modèle de l'onglet précédent.

### Puissance statistique

Cette partie est en cours de développement. L'outil antérieur construit par Mathis Cambreling fonctionne seulement pour le jeu de données ayant servi de base à ses calculs. L'outil n'étant pas généralisable, celui-ci a été retiré pour assurer la stabilité actuelle de l'application. Un autre outil est en cours de développement.<br>

## Bibliographie

**Anderson MJ** (2017) Permutational Multivariate Analysis of Variance (PERMANOVA). Wiley StatsRef: Statistics Reference Online. John Wiley & Sons, Ltd, pp 1–15

**Avezard C, Lavarde P, Pichon A, Legait B, Wallard I** (2017) Impact environnemental et ́economique des activit́es d’exploration ou d’exploitation des ressources minérales marines.

**Bolker BM** (2008) Ecological Models and Data in R. doi: 10.2307/j.ctvcm4g37

**Bolker BM, Brooks ME, Clark CJ, Geange SW, Poulsen JR, Stevens MHH, White J-SS** (2009) Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology & Evolution 24: 127–135

**Colwell R **(2009) Biodiversity: concepts, patterns, and measurement. The Princeton Guide to Ecology. pp 257–263

**David V** (2019) Statistique pour les sciences environnementales. ISTE Editions, Londres, Royaume-Uni

**Gorodetska N, Behaghel G, Dalifard T, Daniel F, Grison X, Hausermann B, Laurent C, De Lantivy S, Lefebvre E, Panonacle H, et al** (2023) L’ ́economie bleue en France.

**Gregorius H-R, Gillet EM **(2008) Generalized Simpson-diversity. Ecological Modelling 211: 90–96

**Legendre P, Gallagher ED** (2001) Ecologically meaningful transformations for ordination of species data. Oecologia 129: 271–280

**Methratta ET** (2020) Monitoring fisheries resources at offshore wind farms: BACI vs. BAG designs. ICES Journal of Marine Science 77: 890–900

**Ministère de l’Environnement de l’́energie et de la mer** (2016) Guide méthodologique pour l'élaboration des documents d’orientations pour une gestion durable des granulats marins (DOGGM). Ministère de l’Environnement, de l’Energie et de la Mer. Paris

**MTE, UNPG, IFREMER, DREAL, DIRM** (2023) Guide technique pour l’élaboration des ́etudes d’impact préalables à la recherche et l’exploitation des granulats marins. 48

**Oksanen J** (2022) Dissimilarity Indices for Community Ecologists.

**Ortiz-Burgos S **(2016) Shannon-Weaver Diversity Index. In MJ Kennish, ed, Encyclopedia of Estuaries. Springer Netherlands, Dordrecht, pp 572–573

**Parent S-E** (2020) Analyse et modélisation d’agroécosystèmes.

**Rassweiler A, Okamoto DK, Reed DC, Kushner DJ, Schroeder DM, Lafferty KD** (2021) Improving the ability of a BACI design to detect impacts within a kelp-forest community. Ecological Applications 31: e02304

**Seger KD, Sousa-Lima R, Schmitter-Soto JJ, Urban ER** (2021) Editorial: Before-After Control-Impact (BACI) Studies in the Ocean. Frontiers in Marine Science 8:

**Shannon CE** (1948) A mathematical theory of communication. The Bell System Technical Journal 27: 379–423

**Smokorowski KE, Randall RG** (2017) Cautions on using the Before-After-Control-Impact design in environmental effects monitoring programs. FACETS 2: 212–232

**Underwood AJ** (1994) On Beyond BACI: Sampling Designs that Might Reliably Detect Environmental Disturbances. Ecological Applications 4: 4–15

**Venables WN, Dichmont CM** (2004) GLMs, GAMs and GLMMs: an overview of theory for applications in fisheries research. Fisheries Research 70: 319–337

**Walker R, Bokuniewicz H, Carlin D, Cato I, Dijkshoorn C, Backer AD, Dalfsen J van, Desprez M, Howe L, Robertsdottir BG, et al** (2016) Effects of extraction of marine sediments on the marine environment 2005-2011. doi: 10.17895/ices.pub.5498

**WGEXT** (2019) Working Group on the Effects of Extraction of Marine Sediments on the Marine Ecosystem (WGEXT). doi: 10.17895/ices.pub.5
